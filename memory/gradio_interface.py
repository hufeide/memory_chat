#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sqlite3
import gradio as gr
import time
import asyncio
from typing import List, Tuple, Optional, Dict
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langgraph_memorey import app, stream_with_timeout, parse_thinking_content

DB_PATH = "ai_memory.db"

def get_formatted_memories(user_id: str) -> str:
    try:
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(
            "SELECT memory_id, content FROM user_memories WHERE user_id = ? ORDER BY updated_at DESC", 
            (user_id,)
        )
        rows = cursor.fetchall()
        conn.close()
        if not rows: return "ğŸ“­ ç›®å‰æ•°æ®åº“ä¸­æ— è®°å½•ã€‚"
        return "\n\n".join([f"ğŸ“Œ {r['memory_id']}\n   â”” {r['content']}" for r in rows])
    except Exception as e:
        return f"è¯»å–è®°å¿†å‡ºé”™: {str(e)}"

# --- çœŸæ­£çš„æµå¼èŠå¤©å‡½æ•° ---
def chat_stream_real(user_id: str, user_input: str, history: List[Dict[str, str]]):
    """çœŸæ­£çš„æµå¼èŠå¤©ï¼Œè¾¹æ¨ç†è¾¹æ‰“å­—"""
    history = history or []
    # åˆå§‹çŠ¶æ€ï¼šç”¨æˆ·è¯´äº†è¯ï¼ŒåŠ©æ‰‹å¼€å§‹å›ç­”
    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": ""})
    
    trace_steps = ["ğŸš€ å¼€å§‹æµå¼æ¨ç†..."]
    
    yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""
    
    start_time = time.time()
    accumulated_content = ""
    thinking_content = ""
    final_answer = ""
    in_thinking = False
    
    try:
        # ä½¿ç”¨çœŸæ­£çš„æµå¼å“åº”
        from langgraph_memorey import get_streaming_response
        
        chunk_count = 0
        for chunk in get_streaming_response(user_id, user_input):
            if chunk:
                chunk_count += 1
                accumulated_content += chunk
                
                # ç®€åŒ–å¤„ç†é€»è¾‘ï¼Œå…ˆä¸å¤„ç†thinkingæ ‡ç­¾
                # ç›´æ¥æ˜¾ç¤ºç´¯ç§¯çš„å†…å®¹
                history[-1]["content"] = accumulated_content
                
                # å®æ—¶æ›´æ–°ç•Œé¢
                elapsed_time = time.time() - start_time
                current_trace = [
                    "ğŸš€ å¼€å§‹æµå¼æ¨ç†...",
                    f"âš¡ å®æ—¶ç”Ÿæˆä¸­... (è€—æ—¶: {elapsed_time:.1f}s)",
                    f"ğŸ“¦ å·²æ”¶åˆ° {chunk_count} ä¸ªchunk",
                    f"ğŸ“ å½“å‰é•¿åº¦: {len(accumulated_content)} å­—ç¬¦"
                ]
                yield history, "\n".join(current_trace), get_formatted_memories(user_id), ""
                
                # æ·»åŠ å°å»¶è¿Ÿï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°æ‰“å­—æ•ˆæœ
                time.sleep(0.02)
        
        # å¤„ç†thinkingæ ‡ç­¾ï¼ˆåœ¨æµå¼å®Œæˆåï¼‰
        if '<thinking>' in accumulated_content and '</thinking>' in accumulated_content:
            thinking_start = accumulated_content.find('<thinking>')
            thinking_end = accumulated_content.find('</thinking>')
            thinking_content = accumulated_content[thinking_start + 10:thinking_end]
            final_answer = accumulated_content[thinking_end + 11:].strip()
            
            # é‡æ–°æ ¼å¼åŒ–æ˜¾ç¤º
            display_content = f"""
<details>
<summary>ğŸ¤” æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€/æŠ˜å )</summary>

{thinking_content}

</details>

**ğŸ’¡ æœ€ç»ˆå›ç­”ï¼š**

{final_answer}"""
            history[-1]["content"] = display_content
        
        # å®Œæˆ
        total_time = time.time() - start_time
        final_trace = [
            "ğŸš€ å¼€å§‹æµå¼æ¨ç†...",
            f"âœ… ç”Ÿæˆå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’",
            f"ğŸ“ æ€»å­—ç¬¦æ•°: {len(accumulated_content)}",
            f"ğŸ“¦ æ€»chunkæ•°: {chunk_count}"
        ]
        
        # ä½¿ç”¨AIåˆ¤æ–­æ˜¯å¦éœ€è¦è®°å¿†æ›´æ–°
        from langgraph_memorey import check_if_needs_memory_update
        has_memory_info = check_if_needs_memory_update(user_input)
        
        if has_memory_info:
            final_trace.append("ğŸ§  AIæ£€æµ‹åˆ°ä¸ªäººä¿¡æ¯ï¼Œæ­£åœ¨åå°æ›´æ–°è®°å¿†...")
            
        yield history, "\n".join(final_trace), get_formatted_memories(user_id), ""
        
        # ç­‰å¾…ä¸€ä¸‹è®©è®°å¿†æ›´æ–°å®Œæˆï¼Œç„¶ååˆ·æ–°è®°å¿†æ˜¾ç¤º
        if has_memory_info:
            time.sleep(3)  # ç»™AIåˆ†æå’Œè®°å¿†æ›´æ–°æ›´å¤šæ—¶é—´
            final_trace[-1] = "âœ… æ™ºèƒ½è®°å¿†æ›´æ–°å®Œæˆ"
            yield history, "\n".join(final_trace), get_formatted_memories(user_id), ""
        
    except Exception as e:
        error_msg = f"âŒ æµå¼ç”Ÿæˆå‡ºé”™: {str(e)}"
        history[-1]["content"] = error_msg
        trace_steps.append(error_msg)
        import traceback
        traceback.print_exc()
        yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""

# --- ä¿ç•™åŸæ¥çš„å‡½æ•°ä½œä¸ºå¤‡ç”¨ ---
def chat_stream_backup(user_id: str, user_input: str, history: List[Dict[str, str]]):
    history = history or []
    # åˆå§‹çŠ¶æ€ï¼šç”¨æˆ·è¯´äº†è¯ï¼ŒåŠ©æ‰‹è¿˜åœ¨æ€è€ƒ
    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": "ğŸ”„ æ­£åœ¨æ€è€ƒ..."})
    
    trace_steps = []
    config = {"configurable": {"user_id": user_id, "thread_id": f"thread_{user_id}"}}
    input_state = {"messages": [HumanMessage(content=user_input)]}

    yield history, "ğŸš€ å·¥ä½œæµå¯åŠ¨...", get_formatted_memories(user_id), ""

    start_time = time.time()
    
    try:
        # ä½¿ç”¨å¸¦è¶…æ—¶çš„æµå¼å¤„ç†
        stream_generator = stream_with_timeout(input_state, config, timeout_seconds=20)
        
        # å¤„ç†æµå¼ç»“æœ
        has_valid_response = False
        chunk_count = 0
        final_content = ""
        
        for stream_result in stream_generator:
            if stream_result is None:
                # å¤„ç†è¶…æ—¶æˆ–é”™è¯¯
                history[-1]["content"] = "â° å¤„ç†æ—¶é—´è¿‡é•¿æˆ–å‡ºç°é”™è¯¯ï¼Œä¸ºäº†æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼Œè¯·é‡æ–°æé—®ã€‚"
                trace_steps.append("âš ï¸ å¤„ç†è¶…æ—¶æˆ–é”™è¯¯")
                yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""
                return
            
            chunk, is_timeout = stream_result
            
            if is_timeout:
                # å¤„ç†è¶…æ—¶æƒ…å†µ
                history[-1]["content"] = "â° æ€è€ƒæ—¶é—´è¿‡é•¿ï¼Œä¸ºäº†æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼Œæˆ‘å°†æä¾›ä¸€ä¸ªå¿«é€Ÿå›ç­”ã€‚å¦‚æœæ‚¨éœ€è¦æ›´è¯¦ç»†çš„åˆ†æï¼Œè¯·é‡æ–°æé—®ã€‚"
                trace_steps.append("âš ï¸ å¤„ç†è¶…æ—¶ (>20ç§’)")
                yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""
                return
            
            chunk_count += 1
            trace_steps.append(f"ğŸ” å¤„ç†å— {chunk_count}")
            
            for node_name, node_data in chunk.items():
                elapsed_time = time.time() - start_time
                trace_steps.append(f"ğŸ“ èŠ‚ç‚¹: {node_name} (è€—æ—¶: {elapsed_time:.1f}s)")
                
                if "messages" in node_data:
                    for j, msg in enumerate(node_data["messages"]):
                        trace_steps.append(f"  ğŸ“ æ¶ˆæ¯ {j+1}: {type(msg).__name__}")
                        if isinstance(msg, AIMessage):
                            if msg.content and msg.content.strip():
                                has_valid_response = True
                                final_content = msg.content
                                trace_steps.append(f"  âœ… æ‰¾åˆ°æœ‰æ•ˆAIå›å¤ï¼Œé•¿åº¦: {len(msg.content)}")
                                
                                # å¼€å§‹æµå¼æ˜¾ç¤º
                                trace_steps.append("  ğŸ¬ å¼€å§‹æµå¼æ˜¾ç¤º...")
                                for partial_content in simulate_streaming_display(msg.content):
                                    history[-1]["content"] = partial_content
                                    yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""
                            else:
                                trace_steps.append(f"  âš ï¸ AIæ¶ˆæ¯å†…å®¹ä¸ºç©º")
                            
                        elif hasattr(msg, "tool_calls") and msg.tool_calls:
                            trace_steps.append(f"ğŸ”§ æ­£åœ¨æå–å’Œå­˜å‚¨äº‹å®ä¿¡æ¯...")
                
                # å®æ—¶æ›´æ–°ç•Œé¢
                if not final_content:  # åªæœ‰åœ¨è¿˜æ²¡æœ‰æœ€ç»ˆå†…å®¹æ—¶æ‰æ›´æ–°
                    yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå›å¤
        if not has_valid_response and history[-1]["content"] == "ğŸ”„ æ­£åœ¨æ€è€ƒ...":
            trace_steps.append("âš ï¸ æœªæ”¶åˆ°æœ‰æ•ˆçš„AIå›å¤")
            history[-1]["content"] = "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ”¶åˆ°æœ‰æ•ˆçš„å›å¤ã€‚è¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚"
        
        total_time = time.time() - start_time
        trace_steps.append(f"âœ… æ€»è€—æ—¶: {total_time:.2f}ç§’")
        yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""

    except Exception as e:
        error_msg = f"âŒ å¤„ç†å‡ºé”™: {str(e)}"
        history[-1]["content"] = error_msg
        trace_steps.append(error_msg)
        yield history, "\n".join(trace_steps), get_formatted_memories(user_id), ""

# --- ç•Œé¢æ„å»º ---
with gr.Blocks(title="AI é•¿æœŸè®°å¿†åŠ©ç†") as demo:
    gr.Markdown("""
    # ğŸ§  LangGraph é•¿æœŸè®°å¿†æ™ºèƒ½ä½“
    
    **åŠŸèƒ½ç‰¹ç‚¹ï¼š**
    - ğŸ”„ **æµå¼è¾“å‡º**ï¼šå®æ—¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”
    - ğŸ¤” **æ€è€ƒè¿‡ç¨‹**ï¼šå¯æŠ˜å çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹
    - â° **è¶…æ—¶ä¿æŠ¤**ï¼šè¶…è¿‡20ç§’è‡ªåŠ¨æä¾›å¿«é€Ÿå›ç­”
    - ğŸ’¾ **é•¿æœŸè®°å¿†**ï¼šè‡ªåŠ¨å­˜å‚¨å’Œç®¡ç†ç”¨æˆ·ä¿¡æ¯
    """)
    
    with gr.Row():
        u_id = gr.Textbox(label="ç”¨æˆ· ID", value="user_001", info="ç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„è®°å¿†")
        
    with gr.Row():
        with gr.Column(scale=3):
            # ä½¿ç”¨åŸç”Ÿçš„èŠå¤©ç•Œé¢ç»„ä»¶
            chatbot = gr.Chatbot(
                label="ğŸ’¬ å¯¹è¯å†å²", 
                height=500,
                show_label=True
            )
            
            with gr.Row():
                msg_in = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯", 
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–å‘Šè¯‰æˆ‘ä¸€äº›å…³äºæ‚¨çš„ä¿¡æ¯... (æŒ‰Enterå‘é€)", 
                    scale=4,
                    lines=1,
                    interactive=True,
                    show_label=False,
                    container=False
                )
                send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary", scale=1, size="sm")
            
            with gr.Row():
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", scale=1)
        
        with gr.Column(scale=2):
            trace_out = gr.Textbox(
                label="ğŸ” æ‰§è¡Œè½¨è¿¹", 
                interactive=False, 
                lines=12,
                info="æ˜¾ç¤ºAIçš„å¤„ç†æ­¥éª¤å’Œè€—æ—¶"
            )
            memo_out = gr.Textbox(
                label="ğŸ§  é•¿æœŸäº‹å®åº“ (SQLite)", 
                interactive=False, 
                lines=15,
                info="AIè®°ä½çš„å…³äºæ‚¨çš„ä¿¡æ¯"
            )

    # æ¸…ç©ºå¯¹è¯åŠŸèƒ½
    def clear_chat():
        return [], "", ""
    
    clear_btn.click(clear_chat, outputs=[chatbot, trace_out, msg_in])
    
    # ç»‘å®šå‘é€æŒ‰é’®
    send_event = send_btn.click(
        chat_stream_real, 
        inputs=[u_id, msg_in, chatbot], 
        outputs=[chatbot, trace_out, memo_out, msg_in]
    )
    
    # ç»‘å®šEnteré”®å‘é€æ¶ˆæ¯ - ç®€åŒ–ç‰ˆæœ¬
    msg_in.submit(
        fn=chat_stream_real,
        inputs=[u_id, msg_in, chatbot], 
        outputs=[chatbot, trace_out, memo_out, msg_in],
        show_progress=True
    )
    
    demo.load(get_formatted_memories, inputs=[u_id], outputs=[memo_out])

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ AI é•¿æœŸè®°å¿†åŠ©ç†...")
    print("ğŸ“ åŠŸèƒ½ç‰¹ç‚¹ï¼š")
    print("   - æµå¼è¾“å‡ºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹")
    print("   - æ€è€ƒå†…å®¹å¯æŠ˜å æŸ¥çœ‹")
    print("   - 20ç§’è¶…æ—¶ä¿æŠ¤æœºåˆ¶")
    print("   - é•¿æœŸè®°å¿†è‡ªåŠ¨ç®¡ç†")
    print(f"ğŸŒ è®¿é—®åœ°å€: http://0.0.0.0:7864")
    
    try:
        demo.launch(
            server_name="0.0.0.0", 
            server_port=8000,  # å†æ¢ä¸ªç«¯å£
            share=False,
            show_error=True,
            theme=gr.themes.Soft() if hasattr(gr, 'themes') else None
        )
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨æˆ–ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")
